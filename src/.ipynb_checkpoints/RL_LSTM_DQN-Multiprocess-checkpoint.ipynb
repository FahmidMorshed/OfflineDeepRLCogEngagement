{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T01:10:31.652972Z",
     "start_time": "2020-12-02T01:10:27.530549Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential, Model\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.python.client import device_lib\n",
    "import sys\n",
    "import keras\n",
    " \n",
    "print(\"TF INFO:\", device_lib.list_local_devices())\n",
    "print('GPU INFO:', K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "K.set_session(tf.Session(config=config))\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T17:48:49.783490Z",
     "start_time": "2020-12-02T17:48:49.719096Z"
    },
    "code_folding": [
     204,
     217,
     316,
     321,
     328,
     350,
     364
    ]
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, df_batch, state_size, action_size,\n",
    "                 minibatch_size=128, gamma=.95, lr=0.001, units=128, priority_aplha = 0.5, \n",
    "                 lookback=3, is_lstm=False, layers=4, bcq=0,\n",
    "                 copy_online_to_target_ep=100, eval_after=100, mode=\"normal\"):\n",
    "        \"\"\"\n",
    "        creates a DQN Agent for batch learning\n",
    "        param: df_batch is the batch data in MDP format\n",
    "        param: state_size\n",
    "        param: action_size\n",
    "        param: minibatch_size \n",
    "        param: gamma\n",
    "        param: lr\n",
    "        param: units\n",
    "        param: priority_aplha for Prioritized Experience Reply. 0 Makes it Vanilla Experience Reply\n",
    "        param: copy_online_to_target_ep copies current network to terget network. meaningless for Double DQN\n",
    "        param: eval_after \n",
    "        param: lookback how many historical states are inluced including the current one\n",
    "        param: is_lstm is for layer type\n",
    "        param: layers is for number of layers including output layer\n",
    "        param: mode is for baselines. normal is no baseline. other options are: random, 0, 1, 2, 3\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #adding priority as noise in batch\n",
    "        df_batch.at[:, 'weight'] = 0.0\n",
    "        for i, row in df_batch.iterrows():\n",
    "            df_batch.at[i, 'priority'] = (np.random.uniform(0, 0.001))**priority_aplha\n",
    "        \n",
    "        # setting parameters\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch = df_batch\n",
    "        \n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = lr\n",
    "        self.units = units\n",
    "        self.priority_aplha = priority_aplha\n",
    "        self.lookback = lookback\n",
    "        self.is_lstm = is_lstm\n",
    "        self.layers = layers\n",
    "        if self.layers<3:\n",
    "            print(\"MIN LAYERS SHOULD BE 3. FORCING 3 LAYERS (including output)\")\n",
    "            self.layers = 3\n",
    "        self.bcq = bcq\n",
    "        \n",
    "        self.copy_online_to_target_ep = copy_online_to_target_ep\n",
    "        self.eval_after = eval_after\n",
    "        \n",
    "        self.batch = self._setup_lookback_states(self.batch)\n",
    "        \n",
    "        self.mode = mode\n",
    "        \n",
    "        # setting up the models\n",
    "        self.model_1 = self._build_model()\n",
    "        self.model_2 = self._build_model()\n",
    "        \n",
    "        \n",
    "        # evaluation variables\n",
    "        self.ecrs = []\n",
    "        self.IS = []\n",
    "        self.WIS = []\n",
    "        self.PDIS = []\n",
    "        self.PDWIS = []\n",
    "        self.DR = []\n",
    "        self.remediations = []\n",
    "    \n",
    "    \n",
    "    def _setup_lookback_states(self, df):\n",
    "        curr_ep = -1\n",
    "        for i, row in df.iterrows():\n",
    "            if curr_ep!=row['episode_id']:\n",
    "                curr_ep = row['episode_id']\n",
    "                prevs = []\n",
    "                for j in range(self.lookback-1):\n",
    "                    prevs.append(np.full(shape=self.state_size, fill_value=0))\n",
    "                state = row['state'] \n",
    "                prevs.append(state)\n",
    "                pervs = deepcopy(prevs)\n",
    "            df.at[i, 'state'] = np.array(prevs)\n",
    "            prevs = deepcopy(prevs[1:])\n",
    "            prevs.append(row['next_state'])\n",
    "            pervs = deepcopy(prevs)\n",
    "            df.at[i, 'next_state'] = np.array(prevs)\n",
    "            prevs = deepcopy(prevs)    \n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            state, next_state = self.get_transformed_state(row)\n",
    "            df.at[i, 'state'] = state\n",
    "            df.at[i, 'next_state'] = next_state\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_transformed_state(self, row):\n",
    "        if self.is_lstm:\n",
    "            state = row['state'].reshape(1, self.lookback, self.state_size)\n",
    "            next_state = row['next_state'].reshape(1, self.lookback, self.state_size)\n",
    "        else:\n",
    "            state = row['state'].reshape(1, self.state_size * self.lookback)\n",
    "            next_state = row['next_state'].reshape(1, self.state_size * self.lookback)\n",
    "        \n",
    "        return state, next_state\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Standard DQN model\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        if self.is_lstm:\n",
    "            # 1 layer\n",
    "            model.add(layers.LSTM(self.units, input_shape=(self.lookback, self.state_size), \n",
    "                                  activation='relu', kernel_regularizer=keras.regularizers.l2(), \n",
    "                                  return_sequences=True, kernel_initializer='glorot_normal'))\n",
    "            \n",
    "            for i in range(self.layers-3):\n",
    "                model.add(layers.LSTM(self.units, kernel_regularizer=keras.regularizers.l2(), \n",
    "                                      return_sequences=True, kernel_initializer='glorot_normal'))\n",
    "            # 1 layer\n",
    "            model.add(layers.LSTM(self.units, activation='relu', kernel_regularizer=keras.regularizers.l2(), \n",
    "                                  return_sequences=False, kernel_initializer='glorot_normal'))\n",
    "        else:\n",
    "            model.add(layers.Dense(self.units, input_dim=self.state_size * self.lookback, activation='relu', \n",
    "                                   kernel_regularizer=keras.regularizers.l2(), kernel_initializer='glorot_normal'))\n",
    "            for i in range(self.layers-2):\n",
    "                model.add(layers.Dense(self.units, activation='relu', \n",
    "                                       kernel_regularizer=keras.regularizers.l2(), kernel_initializer='glorot_normal'))\n",
    "            \n",
    "        # 1 layer\n",
    "        model.add(layers.Dense(self.action_size, activation='linear', \n",
    "                               kernel_regularizer=keras.regularizers.l2(), kernel_initializer='glorot_normal'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate), metrics=[tf.keras.metrics.RootMeanSquaredError(), 'mae'])\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        act_values = self.model_2.predict(state)\n",
    "        return np.argmax(act_values[0]), np.max(act_values[0])\n",
    "    \n",
    "    def state_value(self, state):\n",
    "        act_values = self.model_2.predict(state)\n",
    "        return np.sum(act_values[0]) \n",
    "    \n",
    "    def q_value(self, state, action):\n",
    "        act_values = self.model_2.predict(state)\n",
    "        return act_values[0][action] \n",
    "    \n",
    "    def _filter_bcq(self, row, ns_act_values):\n",
    "        if self.bcq==0:\n",
    "            return np.argmax(ns_act_values)\n",
    "        \n",
    "        gw = self.batch.loc[self.batch['cluster']==row['ns_cluster']].sample(100)\n",
    "        gwa = gw.groupby(['action']).count()['episode_id'].tolist()\n",
    "        gwap = np.array(gwa)/100\n",
    "        \n",
    "        for i, p in enumerate(gwap):\n",
    "            if p<self.bcq:\n",
    "                ns_act_values[i] = -9999\n",
    "        \n",
    "        return np.argmax(ns_act_values)\n",
    "    \n",
    "    def _fit_model(self, row):\n",
    "        i = row.name\n",
    "        state, action, reward, next_state, done = row['state'], row['action'], row['reward'], row['next_state'], row['done']\n",
    "            \n",
    "        target_q = reward\n",
    "        \n",
    "        if not done:    \n",
    "            if self.mode==\"normal\":\n",
    "                ns_act_values = self.model_1.predict(next_state)[0]\n",
    "#                 a_prime = np.argmax(ns_act_values)\n",
    "                a_prime = self._filter_bcq(row, ns_act_values)\n",
    "            elif self.mode==\"random\":\n",
    "                a_prime = np.random.choice(range(self.action_size))\n",
    "            else:\n",
    "                a_prime = int(self.mode)\n",
    "            \n",
    "            target_ns_act_values = self.model_2.predict(next_state)[0]\n",
    "            target_ns_q = target_ns_act_values[a_prime]\n",
    "                           \n",
    "\n",
    "            target_q = reward + self.gamma*target_ns_q\n",
    "\n",
    "            self.batch.loc[i, 'pred_action'] = a_prime\n",
    "            self.batch.loc[i, 'pred_q'] = target_q\n",
    "        \n",
    "        target_f = self.model_1.predict(state)\n",
    "\n",
    "        # Prioritized Experience Reply with noise\n",
    "        \n",
    "        self.batch.loc[i, 'priority'] = (abs(target_q - target_f[0][action]) + np.random.uniform(0, 0.001))**self.priority_aplha\n",
    "\n",
    "        target_f[0][action] = target_q\n",
    "        self.model_1.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "    \n",
    "    def _learn_minibatch(self):\n",
    "        priority_sum = self.batch['priority'].sum()\n",
    "        self.batch['weight'] = self.batch['priority']/priority_sum\n",
    "        minibatch = self.batch.sample(self.minibatch_size, weights=self.batch['weight'])\n",
    "        minibatch.apply(self._fit_model, axis=1) \n",
    "            \n",
    "    \n",
    "    def ecr_reward(self):\n",
    "        reward = 0.0\n",
    "        count = 0\n",
    "        for i, row in self.batch.loc[self.batch['transition_id']==0].iterrows():\n",
    "            state = row['state']\n",
    "            reward += self.act(state)[1]\n",
    "            count += 1\n",
    "            \n",
    "        ecr = reward/count\n",
    "        self.ecrs.append(ecr)\n",
    "        return ecr\n",
    "\n",
    "    \n",
    "    def get_ips(self, action):\n",
    "        if self.action_size==2:\n",
    "            ips = 1.0/0.5\n",
    "        else:\n",
    "            if action == 0 or action == 1:\n",
    "                ips = 1.0/0.1\n",
    "            else:\n",
    "                ips = 1.0/0.4\n",
    "        return ips\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_eval(self):\n",
    "        # set up roh with action_predicted\n",
    "        self.batch['roh'] = -1.0\n",
    "        curr_ep = -1\n",
    "        for i, row in self.batch.iterrows():\n",
    "            if row['episode_id']!=curr_ep:\n",
    "                roh_t = 1\n",
    "                curr_ep = row['episode_id']\n",
    "            if row['action']!=row['pred_action']:\n",
    "                ips = 0\n",
    "            else:\n",
    "                ips = self.get_ips(row['action'])\n",
    "                \n",
    "            roh_t *= ips\n",
    "            self.batch.at[i, 'roh'] = roh_t\n",
    "        \n",
    "        total_eps = len(self.batch['episode_id'].unique())\n",
    "        # equation found in emma brunskill's lecture note\n",
    "        \n",
    "        # as each roh is calculated multiplicatively, the last roh is the entire multiplication result\n",
    "        # summing (gamma**t)*(R_t^i) will result to delayed reward\n",
    "        # we can take the last roh (where done is true) and the delayed reward for each ep\n",
    "        a_is = sum(self.batch.loc[self.batch['done']==True].apply(lambda x: x['roh'] * x['delayed_reward'], axis=1))\n",
    "        isamp = a_is/total_eps\n",
    "        \n",
    "        # for weighted important samplng\n",
    "        sum_roh = sum(self.batch.loc[self.batch['done']==True, 'roh'])\n",
    "        if sum_roh==0:\n",
    "            wis = 0\n",
    "        else:\n",
    "            wis = a_is/sum_roh\n",
    "        \n",
    "        pdis = 0\n",
    "        for transition_id in self.batch['transition_id'].unique():\n",
    "            d = self.batch.loc[self.batch['transition_id']==transition_id]\n",
    "            a_pdis = (self.gamma**transition_id) * sum(d.apply(lambda x: x['roh'] * x['reward'], axis=1))\n",
    "            pdis += (a_pdis/total_eps)\n",
    "        \n",
    "        curr_ep = -1\n",
    "        trans = -1\n",
    "        dr = 0\n",
    "        pdwis = 0\n",
    "        pdwis_nom = 0\n",
    "        pdwis_denom = 0\n",
    "        for i, row in self.batch.iterrows():\n",
    "            if curr_ep!=row['episode_id']:\n",
    "                curr_ep = row['episode_id']\n",
    "                trans = 0\n",
    "            if row['transition_id']!=trans:\n",
    "                print(\"ERROR:\", curr_ep, row['transition_id'], trans)\n",
    "            \n",
    "            gamma_t = self.gamma**row['transition_id']\n",
    "            q_pi_e = self.q_value(row['state'], row['action'])\n",
    "            v_pi_e = self.state_value(row['state'])\n",
    "            roh_t_i = row['roh']\n",
    "            if trans!=0:\n",
    "                roh_t_sub_1_i = self.batch.loc[(self.batch['episode_id']==row['episode_id']) &\n",
    "                                                  (self.batch['transition_id']==(trans-1)), 'roh'].tolist()[0]\n",
    "            else:\n",
    "                roh_t_sub_1_i = 1\n",
    "            \n",
    "            dr += ((gamma_t*roh_t_i*(row['reward'] - q_pi_e)) + (gamma_t*roh_t_sub_1_i*v_pi_e))\n",
    "            \n",
    "            # PDWIS\n",
    "            sum_roh_t = float(sum(self.batch.loc[self.batch['transition_id']==row['transition_id']]['roh']))\n",
    "            if sum_roh_t==0:\n",
    "                w_t_i = 0\n",
    "            else:\n",
    "                w_t_i = row['roh']/sum_roh_t\n",
    "            pdwis_nom += (w_t_i * gamma_t * row['reward'])\n",
    "            pdwis_denom += (w_t_i * gamma_t)\n",
    "            \n",
    "            trans += 1\n",
    "        \n",
    "        dr = dr/total_eps\n",
    "        pdwis = pdwis_nom/pdwis_denom\n",
    "        \n",
    "        \n",
    "        self.IS.append(isamp)\n",
    "        self.WIS.append(wis)\n",
    "        self.PDIS.append(pdis)\n",
    "        self.PDWIS.append(pdwis)\n",
    "        self.DR.append(dr)\n",
    "        \n",
    "        return isamp, wis, pdis, pdwis, dr\n",
    "        \n",
    "    \n",
    "    def predict(self):\n",
    "        self.batch['pred_action'] = -1\n",
    "        self.batch['pred_q'] = 0\n",
    "        self.batch.apply(self._predict_row, axis=1)\n",
    "    \n",
    "    def _predict_row(self, row):\n",
    "        i = row.name\n",
    "        state = row['state']\n",
    "        act, q = self.act(state)\n",
    "        self.batch.loc[i, 'pred_action'] = act\n",
    "        self.batch.loc[i, 'pred_q'] = q\n",
    "    \n",
    "    def learn(self, epoch):\n",
    "        for i in range(epoch):\n",
    "            self._learn_minibatch()\n",
    "            \n",
    "            if (i+1)%self.copy_online_to_target_ep==0:\n",
    "                self.model_2.set_weights(self.model_1.get_weights())\n",
    "            \n",
    "            if (i+1)%self.eval_after==0:\n",
    "                t1 = time.time()\n",
    "                self.predict()\n",
    "                \n",
    "                ecr = self.ecr_reward()\n",
    "                isamp, wis, pdis, pdwis, dr = self.get_eval()\n",
    "                t2 = time.time()\n",
    "                print(\"Eval Time:\", (t2-t1))\n",
    "                print(\"--epoch: {}/{} | ECR: {:.5f} | IS: {:.5f} | WIS: {:.5f} | PDIS: {:.5f} | PDWIS: {:.5f} | DR: {:.5f} --\".format(i+1, epoch, ecr, isamp, wis, pdis, pdwis, dr))\n",
    "                self.summary()\n",
    "        \n",
    "        self.model_2.set_weights(self.model_1.get_weights())\n",
    "        self.predict()\n",
    "                \n",
    "    \n",
    "    def get_all_eval_df(self):\n",
    "        eval_df = pd.DataFrame(columns=['ECR', 'IS', 'WIS', 'PDIS', 'PDWIS', 'DR', 'REMEDIATION'])\n",
    "        \n",
    "        eval_df['ECR'] = self.ecrs\n",
    "        eval_df['IS'] = self.IS\n",
    "        eval_df['WIS'] = self.WIS\n",
    "        eval_df['PDIS'] = self.PDIS\n",
    "        eval_df['PDWIS'] = self.PDWIS\n",
    "        eval_df['DR'] = self.DR\n",
    "        eval_df['REMEDIATION'] = self.remediations\n",
    "        \n",
    "        return eval_df\n",
    "    \n",
    "    \n",
    "    def summary(self):\n",
    "        pred_const = len(self.batch.loc[self.batch['pred_action'] == 3]) \n",
    "        pred_active = len(self.batch.loc[self.batch['pred_action'] == 2])\n",
    "        pred_pass = len(self.batch.loc[self.batch['pred_action'] == 1])\n",
    "        pred_none = len(self.batch.loc[self.batch['pred_action'] == 0])\n",
    "\n",
    "        self.remediations.append({\"constructive\": pred_const, \"active\": pred_active, \"passive\": pred_pass,\n",
    "                                \"none\": pred_none})\n",
    "        print(\"Pred-> Constructive: {}, Active: {}, Passive: {}, None: {}\"\n",
    "              .format(pred_const, pred_active, pred_pass, pred_none))\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T17:48:52.705795Z",
     "start_time": "2020-12-02T17:48:52.694705Z"
    }
   },
   "outputs": [],
   "source": [
    "def summary_result(df):\n",
    "\n",
    "    true_const = len(df.loc[df['action'] == 3])\n",
    "    true_active = len(df.loc[df['action'] == 2])\n",
    "    true_pass = len(df.loc[df['action'] == 1])\n",
    "    true_none = len(df.loc[df['action'] == 0])\n",
    "    \n",
    "    pred_const = len(df.loc[df['pred_action'] == 3]) \n",
    "    pred_active = len(df.loc[df['pred_action'] == 2])\n",
    "    pred_pass = len(df.loc[df['pred_action'] == 1])\n",
    "    pred_none = len(df.loc[df['pred_action'] == 0])\n",
    "     \n",
    "    \n",
    "    print(\"True-> Constructive: {}, Active: {}, Passive: {}, None: {}\"\n",
    "          .format(true_const, true_active, true_pass, true_none))\n",
    "    print(\"Pred-> Constructive: {}, Active: {}, Passive: {}, None: {}\"\n",
    "          .format(pred_const, pred_active, pred_pass, pred_none))\n",
    "    \n",
    "    \n",
    "    \n",
    "    true_reward = df.loc[df['done']==True]['reward']\n",
    "    pred_reward = df.loc[df['transition_id']==0]['pred_q']\n",
    "    true_reward_mean = np.mean(true_reward)\n",
    "    true_reward_std = np.std(true_reward)\n",
    "    pred_reward_mean = np.mean(pred_reward)\n",
    "    pred_reward_std = np.std(pred_reward)\n",
    "    \n",
    "    print(\"-> True Reward: {:.5f}/{:.5f}, Pred Reward: {:.5f}/{:.5f}\".format(\n",
    "        true_reward_mean, true_reward_std, pred_reward_mean, pred_reward_std))\n",
    "\n",
    "    \n",
    "    ret_dict = {\"True\": {\"Constructive\": true_const, \"Active\": true_active, \"Passive\": true_pass, \n",
    "                         \"None\": true_none, \"RewardMean\": true_reward_mean, \"RewardStd\": true_reward_std},\n",
    "               \"Pred\": {\"Constructive\": pred_const, \"Active\": pred_active, \"Passive\": pred_pass, \n",
    "                         \"None\": pred_none, \"RewardMean\": pred_reward_mean, \"RewardStd\": pred_reward_std}\n",
    "               }\n",
    "    \n",
    "    \n",
    "    \n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T17:51:55.181669Z",
     "start_time": "2020-12-02T17:51:55.175324Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(agent, run_name, epoch):\n",
    "    print(\"===== STARTING \", run_name, \"=====\")\n",
    "    agent.learn(epoch)\n",
    "    result_df = agent.batch\n",
    "    eval_df = agent.get_all_eval_df()\n",
    "    eval_df.to_pickle(result_dir + run_name +'_eval.pkl')\n",
    "    summary_result(result_df)\n",
    "    result_df.to_pickle(result_dir + run_name +'_result.pkl')\n",
    "\n",
    "    return result_df, eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T17:52:11.454854Z",
     "start_time": "2020-12-02T17:52:08.697637Z"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "def multi_RL(random_state):\n",
    "    from keras.models import Sequential, Model\n",
    "    import keras.layers as layers\n",
    "    from keras.optimizers import Adam\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "\n",
    "    epoch = 200\n",
    "    is_lstm = False\n",
    "    lookback = 1\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    random.seed(random_state)\n",
    "    tf.set_random_seed(random_state)\n",
    "    \n",
    "    \n",
    "    \n",
    "    run_name = \"DQN_is_lstm_\"+ str(is_lstm) + \"_lookback_\" + str(lookback) + \"_run_\" + str(epoch) + \"_rs_\" + str(random_state)\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    K.set_session(tf.Session(config=config))\n",
    "    K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "    \n",
    "    df_org = pd.read_pickle('../temp/df_all_norm_cluster.pkl')\n",
    "    df = df_org.copy()\n",
    "    epoch = 200\n",
    "    df['reward'] = df['delayed_reward']\n",
    "    result_dir = '../trash/'\n",
    "    \n",
    "#     agent = DQNAgent(df_batch=df.copy(), state_size=len(df.iloc[0]['state']), action_size=4, \n",
    "#                              copy_online_to_target_ep=100, eval_after=100, bcq=0.1,\n",
    "#                              lookback=lookback, layers=4, is_lstm=is_lstm, mode=\"normal\")\n",
    "#     result_df, eval_df = run(agent, run_name, epoch)\n",
    "    \n",
    "    with open(result_dir+run_name + '.txt', 'w') as f:\n",
    "        for i in range(100):\n",
    "            f.write(str(i))\n",
    "        \n",
    "with Pool() as pool: \n",
    "    r = pool.map(multi_RL, [rs for rs in [1,2,3,4,5]])\n",
    "    pool.join()\n",
    "    pool.close()\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
