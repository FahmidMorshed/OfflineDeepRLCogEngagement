{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T20:49:18.297406Z",
     "start_time": "2020-11-04T20:49:18.290657Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pdb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers.merge import _Merge, Multiply\n",
    "import ast\n",
    "import gym\n",
    "\n",
    "random_state=0\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T20:49:19.392295Z",
     "start_time": "2020-11-04T20:49:19.343411Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class QLayer(_Merge):\n",
    "    \"\"\"\n",
    "    Q Layer that merges an advantage and value layer'''\n",
    "    Needed for dueling dqn only\n",
    "    \"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        '''Assume that the inputs come in as [value, advantage]'''\n",
    "        output = inputs[0] + (inputs[1] - K.mean(inputs[1], axis=1, keepdims=True))\n",
    "        return output\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, df_batch, state_size, action_size, \n",
    "                 minibatch_size=64, gamma=.95, lr=0.0001, units=128, hidden_layers=1,\n",
    "                 dueling=False, double_param=0, priority_alpha=0,\n",
    "                 copy_online_to_target_ep=100, eval_after=100):\n",
    "        \n",
    "        # NOT FOR NOW. FUTURE WORK\n",
    "        #adding priority as noise in batch\n",
    "        df_batch.at[:, 'weight'] = 0.0\n",
    "        for i, row in df_batch.iterrows():\n",
    "            df_batch.at[i, 'priority'] = (0 + np.random.uniform(0, 0.001))**priority_alpha\n",
    "\n",
    "        \n",
    "        # setting parameters\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch = df_batch\n",
    "        \n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = lr\n",
    "        self.units = units\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        self.dueling = dueling\n",
    "        self.double_param = double_param\n",
    "        self.priority_alpha = priority_alpha\n",
    "        \n",
    "        self.copy_online_to_target_ep = copy_online_to_target_ep\n",
    "        self.eval_after = eval_after\n",
    "        \n",
    "        \n",
    "        # setting up the models\n",
    "        if self.dueling:\n",
    "            # TODO\n",
    "            self.model_1 = self._build_model_dueling()\n",
    "            self.model_2 = self._build_model_dueling()\n",
    "        else:\n",
    "            self.model_1 = self._build_model()\n",
    "            self.model_2 = self._build_model()\n",
    "        \n",
    "        # evaluation variables\n",
    "        self.R = []\n",
    "        self.ecrs = []\n",
    "    \n",
    "    def _build_model_dueling(self):\n",
    "        inputs = layers.Input(shape=(self.state_size,))\n",
    "        z = layers.Dense(self.units, kernel_initializer='glorot_normal', activation='relu')(inputs)\n",
    "        for layer in range(self.hidden_layers-1):\n",
    "            z = layers.Dense(self.units, kernel_initializer='glorot_normal', activation='relu')(z)\n",
    "            \n",
    "        value = layers.Dense(1, kernel_initializer='glorot_normal', activation='linear')(z)\n",
    "        \n",
    "        adv = layers.Dense(self.action_size, kernel_initializer='glorot_normal', activation='linear')(z)\n",
    "\n",
    "        q = QLayer()([value, adv])\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=q)\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Standard DQN model\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(layers.Dense(self.units, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_normal'))\n",
    "        for layer in range(self.hidden_layers-1):\n",
    "            model.add(layers.Dense(self.units, activation='relu', kernel_initializer='glorot_normal'))\n",
    "        \n",
    "        model.add(layers.Dense(self.action_size, activation='linear', kernel_initializer='glorot_normal'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate), metrics=[tf.keras.metrics.RootMeanSquaredError(), 'mae'])\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        state_array = np.array(state.reshape(1, self.state_size))\n",
    "        act_values = self.model_2.predict(state_array)\n",
    "        return np.argmax(act_values[0]), np.max(act_values[0])\n",
    "    \n",
    "    def learn(self, epoch, env=None):\n",
    "        for i in range(epoch):\n",
    "            self._learn_minibatch()\n",
    "            print()\n",
    "            if (i+1)%self.copy_online_to_target_ep==0:\n",
    "                self.model_2.set_weights(self.model_1.get_weights())\n",
    "            \n",
    "            if (i+1)%self.eval_after==0:\n",
    "                r = self.run_env(env)\n",
    "                self.R.append(r)\n",
    "                ecr = 0\n",
    "#                 ecr = self.ecr_reward()\n",
    "                print(\"--epoch: {}/{} | ECR: {:.5f} | R: {:.2f} --\".format(i+1, epoch, ecr, r))\n",
    "        \n",
    "        print(\"--final run--\")\n",
    "        self.model_2 = self.model_1\n",
    "        r = self.run_env(env)\n",
    "        self.R.append(r)\n",
    "        self.predict()\n",
    "        ecr = self.ecr_reward()\n",
    "        print(\"--epoch: {}/{} | ECR: {:.5f} | R: {:.2f} --\".format(i+1, epoch, ecr, r))\n",
    "    \n",
    "    \n",
    "    def _learn_row(self, row):\n",
    "        i = row.name\n",
    "        state, action, reward, next_state, done = row['state'], row['action'], row['reward'], row['next_state'], row['done']\n",
    "\n",
    "        target_q = reward\n",
    "\n",
    "        # For Double DQN\n",
    "        rand = random.random()\n",
    "        \n",
    "        if rand >= self.double_param:\n",
    "            if not done: \n",
    "                ns_act_values = self.model_1.predict(next_state.reshape(1,self.state_size))[0]\n",
    "                a_prime = np.argmax(ns_act_values)\n",
    "\n",
    "                target_ns_act_values = self.model_2.predict(next_state.reshape(1,self.state_size))[0]\n",
    "                target_ns_q = target_ns_act_values[a_prime]\n",
    "\n",
    "                target_q = reward + self.gamma*target_ns_q\n",
    "\n",
    "                self.batch.at[i, 'pred_action'] = a_prime\n",
    "                self.batch.at[i, 'pred_reward'] = target_q\n",
    "                \n",
    "            target_f = self.model_1.predict(state.reshape(1,self.state_size))\n",
    "            # Prioritized Experience Reply with noise\n",
    "            self.batch.loc[i, 'priority'] = (abs(target_q - target_f[0][action]) + np.random.uniform(0, 0.001))**self.priority_alpha\n",
    "\n",
    "            target_f[0][action] = target_q\n",
    "            self.model_1.fit(state.reshape(1,self.state_size), target_f, epochs=1, verbose=0)\n",
    "        else:\n",
    "            if not done: \n",
    "                ns_act_values = self.model_2.predict(next_state.reshape(1,self.state_size))[0]\n",
    "                a_prime = np.argmax(ns_act_values)\n",
    "\n",
    "                target_ns_act_values = self.model_1.predict(next_state.reshape(1,self.state_size))[0]\n",
    "                target_ns_q = target_ns_act_values[a_prime]\n",
    "\n",
    "                target_q = reward + self.gamma*target_ns_q\n",
    "\n",
    "                self.batch.at[i, 'pred_action'] = a_prime\n",
    "                self.batch.at[i, 'pred_reward'] = target_q\n",
    "\n",
    "            target_f = self.model_2.predict(state.reshape(1,self.state_size))\n",
    "            # Prioritized Experience Reply with noise\n",
    "            self.batch.loc[i, 'priority'] = (abs(target_q - target_f[0][action]) + np.random.uniform(0, 0.001))**self.priority_alpha\n",
    "\n",
    "            target_f[0][action] = target_q\n",
    "            self.model_2.fit(state.reshape(1,self.state_size), target_f, epochs=1, verbose=0)\n",
    "            \n",
    "                \n",
    "    \n",
    "    def _learn_minibatch(self):\n",
    "        # For PER\n",
    "        priority_sum = self.batch['priority'].sum()\n",
    "        self.batch['weight'] = self.batch['priority']/priority_sum\n",
    "        \n",
    "        \n",
    "        minibatch = self.batch.sample(self.minibatch_size, weights=self.batch['weight'])\n",
    "        minibatch.apply(self._learn_row, axis=1)\n",
    "        \n",
    "            \n",
    "    def ecr_reward(self):\n",
    "        reward = 0.0\n",
    "        count = 0\n",
    "        for i, row in self.batch.loc[self.batch['transition_id']==1].iterrows():\n",
    "            state = row['state']\n",
    "            next_state = row['next_state']\n",
    "                \n",
    "            reward += self.act(state)[1]\n",
    "            count += 1\n",
    "            \n",
    "        ecr = reward/count\n",
    "        self.ecrs.append(ecr)\n",
    "        return ecr\n",
    "    \n",
    "    def _predict_row(self, row):\n",
    "        i = row.name\n",
    "        state = row['state']\n",
    "        next_state = row['next_state']\n",
    "\n",
    "        act, q = self.act(state)\n",
    "        self.batch.loc[i, 'pred_action'] = act\n",
    "        self.batch.loc[i, 'pred_reward'] = q\n",
    "        \n",
    "    def predict(self):\n",
    "        self.batch.apply(self._predict_row, axis=1)\n",
    "        \n",
    "        return self.batch\n",
    "    \n",
    "    def run_env(self, env, cast_np=True):\n",
    "        if env is None:\n",
    "            return 0\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            if cast_np:\n",
    "                action = self.act(np.array(state))[0]\n",
    "            else:\n",
    "                action = self.act(state)[0]\n",
    "            if action not in env.actions_allowed():\n",
    "                action = np.random.choice(env.actions_allowed())\n",
    "                print(\"Random action\")\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                return total_reward\n",
    "            \n",
    "    def get_all_eval_df(self):\n",
    "        eval_df = pd.DataFrame(columns=['ECR', 'R'])\n",
    "        \n",
    "        eval_df['ECR'] = self.ecrs\n",
    "        eval_df['R'] = self.R\n",
    "        \n",
    "        return eval_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T20:49:21.076312Z",
     "start_time": "2020-11-04T20:49:20.950306Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridworld | Total transitions: 29404  | Total episodes: 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transition_id</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>immediate_reward</th>\n",
       "      <th>delayed_reward</th>\n",
       "      <th>done</th>\n",
       "      <th>next_state</th>\n",
       "      <th>infer_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>-0.042425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>-0.061948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>-0.081035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>-0.261603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 2]</td>\n",
       "      <td>-0.196726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29399</th>\n",
       "      <td>999</td>\n",
       "      <td>32</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>-0.090252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29400</th>\n",
       "      <td>999</td>\n",
       "      <td>33</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>-0.055686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29401</th>\n",
       "      <td>999</td>\n",
       "      <td>34</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>-0.150876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29402</th>\n",
       "      <td>999</td>\n",
       "      <td>35</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>-0.071085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29403</th>\n",
       "      <td>999</td>\n",
       "      <td>36</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>True</td>\n",
       "      <td>[13, 6]</td>\n",
       "      <td>-0.053962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29404 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_id transition_id    state action  immediate_reward  \\\n",
       "0              0             0   [0, 0]      0              -0.1   \n",
       "1              0             1   [0, 1]      1              -1.1   \n",
       "2              0             2   [1, 1]      0               0.9   \n",
       "3              0             3   [1, 2]      1              -0.1   \n",
       "4              0             4   [2, 2]      1              -0.1   \n",
       "...          ...           ...      ...    ...               ...   \n",
       "29399        999            32  [12, 6]      2              -0.1   \n",
       "29400        999            33  [12, 6]      2              -0.1   \n",
       "29401        999            34  [12, 6]      2              -0.1   \n",
       "29402        999            35  [12, 6]      2              -0.1   \n",
       "29403        999            36  [12, 6]      1              -0.1   \n",
       "\n",
       "       delayed_reward   done next_state  infer_reward  \n",
       "0                 0.0  False     [0, 1]     -0.042425  \n",
       "1                 0.0  False     [1, 1]     -0.061948  \n",
       "2                 0.0  False     [1, 2]     -0.081035  \n",
       "3                 0.0  False     [2, 2]     -0.261603  \n",
       "4                 0.0  False     [3, 2]     -0.196726  \n",
       "...               ...    ...        ...           ...  \n",
       "29399             0.0  False    [12, 6]     -0.090252  \n",
       "29400             0.0  False    [12, 6]     -0.055686  \n",
       "29401             0.0  False    [12, 6]     -0.150876  \n",
       "29402             0.0  False    [12, 6]     -0.071085  \n",
       "29403            -2.7   True    [13, 6]     -0.053962  \n",
       "\n",
       "[29404 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('../data/gridworldchi_1k_inf.pkl')\n",
    "print(\"Gridworld\", \"| Total transitions:\", len(df), \" | Total episodes:\", len(df['episode_id'].unique()))\n",
    "df_org = df.copy()\n",
    "df_org\n",
    "\n",
    "df_org['state'] = df_org['state'].apply(np.array)\n",
    "df_org['next_state'] = df_org['next_state'].apply(np.array)\n",
    "df_org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T21:16:02.781257Z",
     "start_time": "2020-11-04T20:49:22.128996Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==gridworld_ep_size_10000_infer_reward_dueling_False_double_0_priority_0.05_rs_0_==\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "Random action\n",
      "--epoch: 100/10000 | ECR: 0.00000 | R: -2.40 --\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aa58f74ff892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m                  copy_online_to_target_ep=100, eval_after=100)\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2cef04ecbba3>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, epoch, env)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_after\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mecr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2cef04ecbba3>\u001b[0m in \u001b[0;36mrun_env\u001b[0;34m(self, env, cast_np)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcast_np\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2cef04ecbba3>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mstate_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mact_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    321\u001b[0m                     \u001b[0;34m'forgot to call `super(YourClass, self).__init__()`.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     ' Always start with this line.')\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0m_DISABLE_TRACKING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_DISABLE_TRACKING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_metrics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym_gridworld\n",
    "import gym_gridworldchi\n",
    "\n",
    "env = gym.make('gridworldchi-v0')\n",
    "\n",
    "result_dir = '../temp/'\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# env.max_episode_steps = 1500\n",
    "epoch = 10000\n",
    "action_size = 3\n",
    "env_name = 'gridworld'\n",
    "prefix = ''\n",
    "\n",
    "random_state=0\n",
    "df = df_org.copy()\n",
    "df['reward'] = df['infer_reward']\n",
    "        \n",
    "dueling, double_param, priority_alpha = (False, 0, .05)\n",
    "            \n",
    "prefix = env_name + '_' + 'ep_size_' + str(10000) + '_' + 'infer_reward' + '_' + \\\n",
    "    'dueling_' + str(dueling) + '_double_' + str(double_param) + '_priority_' + \\\n",
    "    str(priority_alpha) + '_' + 'rs_' + str(random_state) + '_'\n",
    "            \n",
    "        \n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "env.seed(random_state)\n",
    "\n",
    "print(\"==\" + prefix + \"==\")\n",
    "agent = DQNAgent(df_batch=df, state_size=len(df.iloc[0]['state']), action_size=action_size, \n",
    "                 dueling=dueling, double_param=double_param, priority_alpha=priority_alpha,\n",
    "                 copy_online_to_target_ep=100, eval_after=100)\n",
    "\n",
    "agent.learn(epoch, env)\n",
    "\n",
    "\n",
    "result = agent.batch\n",
    "eval_df = agent.get_all_eval_df()\n",
    "eval_df.to_pickle(result_dir + prefix +'eval.pkl')\n",
    "result.to_pickle(result_dir + prefix +'result.pkl')\n",
    "eval_df.to_csv(result_dir + prefix +'eval.csv')\n",
    "result.to_csv(result_dir + prefix +'result.csv')\n",
    "\n",
    "print('==run ends==')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T06:38:48.562467Z",
     "start_time": "2020-11-04T05:45:32.424526Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym_gridworld\n",
    "\n",
    "env = gym.make('gridworld-v0')\n",
    "\n",
    "result_dir = '../temp/'\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# env.max_episode_steps = 1500\n",
    "epoch = 10000\n",
    "action_size = 2\n",
    "env_name = 'gridworld_imm_'\n",
    "prefix = ''\n",
    "\n",
    "random_state=0\n",
    "df = df_org.copy()\n",
    "df['reward'] = df['immediate_reward']\n",
    "        \n",
    "dueling, double_param, priority_alpha = (False, 0, 0.05)\n",
    "            \n",
    "prefix = env_name + '_' + 'ep_size_' + str(10000) + '_' + 'infer_reward' + '_' + \\\n",
    "    'dueling_' + str(dueling) + '_double_' + str(double_param) + '_priority_' + \\\n",
    "    str(priority_alpha) + '_' + 'rs_' + str(random_state) + '_'\n",
    "            \n",
    "        \n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "env.seed(random_state)\n",
    "\n",
    "print(\"==\" + prefix + \"==\")\n",
    "agent = DQNAgent(df_batch=df, state_size=len(df.iloc[0]['state']), action_size=action_size, \n",
    "                 dueling=dueling, double_param=double_param, priority_alpha=priority_alpha,\n",
    "                 copy_online_to_target_ep=100, eval_after=100)\n",
    "\n",
    "agent.learn(epoch, env)\n",
    "\n",
    "\n",
    "result = agent.batch\n",
    "eval_df = agent.get_all_eval_df()\n",
    "eval_df.to_pickle(result_dir + prefix +'eval.pkl')\n",
    "result.to_pickle(result_dir + prefix +'result.pkl')\n",
    "eval_df.to_csv(result_dir + prefix +'eval.csv')\n",
    "result.to_csv(result_dir + prefix +'result.csv')\n",
    "\n",
    "print('==run ends==')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T07:36:45.645805Z",
     "start_time": "2020-11-04T06:38:48.566869Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym_gridworld\n",
    "\n",
    "env = gym.make('gridworld-v0')\n",
    "\n",
    "result_dir = '../temp/'\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# env.max_episode_steps = 1500\n",
    "epoch = 10000\n",
    "action_size = 2\n",
    "env_name = 'gridworld_del_'\n",
    "prefix = ''\n",
    "\n",
    "random_state=0\n",
    "df = df_org.copy()\n",
    "df['reward'] = df['delayed_reward']\n",
    "        \n",
    "dueling, double_param, priority_alpha = (False, 0, 0.05)\n",
    "            \n",
    "prefix = env_name + '_' + 'ep_size_' + str(10000) + '_' + 'infer_reward' + '_' + \\\n",
    "    'dueling_' + str(dueling) + '_double_' + str(double_param) + '_priority_' + \\\n",
    "    str(priority_alpha) + '_' + 'rs_' + str(random_state) + '_'\n",
    "            \n",
    "        \n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "env.seed(random_state)\n",
    "\n",
    "print(\"==\" + prefix + \"==\")\n",
    "agent = DQNAgent(df_batch=df, state_size=len(df.iloc[0]['state']), action_size=action_size, \n",
    "                 dueling=dueling, double_param=double_param, priority_alpha=priority_alpha,\n",
    "                 copy_online_to_target_ep=100, eval_after=100)\n",
    "\n",
    "agent.learn(epoch, env)\n",
    "\n",
    "\n",
    "result = agent.batch\n",
    "eval_df = agent.get_all_eval_df()\n",
    "eval_df.to_pickle(result_dir + prefix +'eval.pkl')\n",
    "result.to_pickle(result_dir + prefix +'result.pkl')\n",
    "eval_df.to_csv(result_dir + prefix +'eval.csv')\n",
    "result.to_csv(result_dir + prefix +'result.csv')\n",
    "\n",
    "print('==run ends==')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
